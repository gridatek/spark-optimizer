name: AWS EMR Integration Test

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region'
        required: true
        default: 'us-west-2'
      cluster_id:
        description: 'EMR Cluster ID (optional - will list all if not provided)'
        required: false
      max_clusters:
        description: 'Maximum clusters to collect from'
        required: false
        default: '5'

jobs:
  test-emr-integration:
    name: Test AWS EMR Connection and Data Collection
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.13'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[aws]"

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ inputs.aws_region }}

    - name: Verify AWS Access
      run: |
        echo "Testing AWS credentials and EMR access..."
        aws sts get-caller-identity
        aws emr list-clusters --region ${{ inputs.aws_region }} --active | head -20

    - name: Setup database
      run: |
        python scripts/setup_db.py
        ls -la *.db

    - name: Test EMR Collector (List Clusters)
      if: ${{ !inputs.cluster_id }}
      run: |
        echo "Testing EMR collector with cluster discovery..."
        python -c "
        from spark_optimizer.collectors.emr_collector import EMRCollector

        collector = EMRCollector(
            region='${{ inputs.aws_region }}',
            config={
                'max_clusters': int('${{ inputs.max_clusters }}'),
                'collect_costs': True
            }
        )

        print('✓ EMR Collector initialized successfully')
        print(f'Region: {collector.region}')
        print(f'Max clusters: {collector.max_clusters}')

        # Validate configuration
        if collector.validate_config():
            print('✓ EMR connection validated')
        else:
            print('✗ EMR connection validation failed')
            exit(1)

        # Collect data
        print('Starting data collection...')
        jobs = collector.collect()
        print(f'✓ Collected data for {len(jobs)} applications')

        if jobs:
            print('\\nSample application:')
            print(f'  App ID: {jobs[0].get(\"app_id\")}')
            print(f'  Name: {jobs[0].get(\"app_name\")}')
            print(f'  Cluster: {jobs[0].get(\"cluster_type\")}')
            print(f'  Status: {jobs[0].get(\"status\")}')
        "

    - name: Test EMR Collector (Specific Cluster)
      if: ${{ inputs.cluster_id }}
      run: |
        echo "Testing EMR collector with cluster ID: ${{ inputs.cluster_id }}"
        python -c "
        from spark_optimizer.collectors.emr_collector import EMRCollector

        collector = EMRCollector(
            region='${{ inputs.aws_region }}',
            config={
                'cluster_ids': ['${{ inputs.cluster_id }}'],
                'collect_costs': True
            }
        )

        print('✓ EMR Collector initialized')

        # Collect data
        jobs = collector.collect()
        print(f'✓ Collected data for {len(jobs)} applications from cluster ${{ inputs.cluster_id }}')

        if jobs:
            for i, job in enumerate(jobs[:3]):  # Show first 3
                print(f'\\nApplication {i+1}:')
                print(f'  App ID: {job.get(\"app_id\")}')
                print(f'  Name: {job.get(\"app_name\")}')
                print(f'  Duration: {job.get(\"duration_ms\")}ms')
        "

    - name: Save to Database
      run: |
        echo "Saving collected data to database..."
        python -c "
        from spark_optimizer.collectors.emr_collector import EMRCollector
        from spark_optimizer.storage.database import Database

        collector = EMRCollector(
            region='${{ inputs.aws_region }}',
            config={'max_clusters': int('${{ inputs.max_clusters }}')}
        )

        db = Database('sqlite:///spark_optimizer.db')
        jobs = collector.collect()

        saved = 0
        for job in jobs:
            try:
                db.save_job(job)
                saved += 1
            except Exception as e:
                print(f'Error saving job: {e}')

        print(f'✓ Saved {saved}/{len(jobs)} jobs to database')
        "

    - name: Verify Database
      run: |
        python -c "
        from spark_optimizer.storage.database import Database
        from spark_optimizer.storage.models import SparkApplication

        db = Database('sqlite:///spark_optimizer.db')
        with db.get_session() as session:
            count = session.query(SparkApplication).count()
            print(f'Total applications in database: {count}')

            apps = session.query(SparkApplication).limit(5).all()
            print(f'\\nRecent applications:')
            for app in apps:
                print(f'  - {app.app_id}: {app.app_name} ({app.cluster_type})')
        "

    - name: Generate Summary
      if: always()
      run: |
        echo "## AWS EMR Integration Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Region:** ${{ inputs.aws_region }}" >> $GITHUB_STEP_SUMMARY
        echo "**Max Clusters:** ${{ inputs.max_clusters }}" >> $GITHUB_STEP_SUMMARY
        if [ -n "${{ inputs.cluster_id }}" ]; then
          echo "**Cluster ID:** ${{ inputs.cluster_id }}" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f spark_optimizer.db ]; then
          python -c "
        from spark_optimizer.storage.database import Database
        from spark_optimizer.storage.models import SparkApplication

        db = Database('sqlite:///spark_optimizer.db')
        with db.get_session() as session:
            count = session.query(SparkApplication).count()
            print(f'**Applications Collected:** {count}')
        " >> $GITHUB_STEP_SUMMARY
        fi

    - name: Upload database artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: emr-test-database
        path: spark_optimizer.db
        retention-days: 7
